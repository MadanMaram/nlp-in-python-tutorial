{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOunPNrFwlQob8DnSyLvxkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadanMaram/nlp-in-python-tutorial/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHM7kpCkTOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TWinFwRtLW4",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "The ability to categorize opinions expressed in the text of tweets—and especially to determine whether the writer's attitude is positive, negative, or neutral—is highly valuable. In this guide, we will use the process known as sentiment analysis to categorize the opinions of people on Twitter towards a hypothetical topic called #hashtag.\n",
        "\n",
        "There are different ordinal scales used to categorize tweets. A five-point ordinal scale includes five categories: Highly Negative, Slightly Negative, Neutral, Slightly Positive, and Highly Positive. A three-point ordinal scale includes Negative, Neutral, and Positive; and a two-point ordinal scale includes Negative and Positive. In this guide, we will use a three-point ordinal scale to categorize tweets with #hashtag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNVZ2FcMndOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed815310-eb73-43bd-e616-34d69f491aaa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip-b8BvJyEgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "082295d7-c65d-43d7-df15-f851cba90eb7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL04oqowkfoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Data Preprocessing and Feature Engineering\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "#Model Selection and Validation\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SXMnru_uwl9",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset\n",
        "After you download the CSV, you'll see that there are 1.6 million tweets already coded into three categories by hand.\n",
        "\n",
        "This dataset encoded the target variable with a 3-point ordinal scale: 0 = negative, 2 = neutral, 4 = positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwKyHR2owuti",
        "colab_type": "text"
      },
      "source": [
        "The dataset has six columns 'target', 't_id', 'created_at', 'query', 'user', 'text', but we are only interested in 'text', 'target'. You can include other columns also if you like. To make it scalable, you need a small script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUCWsibww2En",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing Tweets\n",
        "This is one of the essential steps in any natural language processing (NLP) task. Data scientists never get filtered, ready-to-use data. To make it workable, there is a lot of processing that needs to happen.\n",
        "\n",
        "Letter casing: Converting all letters to either upper case or lower case.\n",
        "Tokenizing: Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
        "Noise removal: Eliminating unwanted characters, such as HTML tags, punctuation marks, special characters, white spaces etc.\n",
        "Stopword removal: Some words do not contribute much to the machine learning model, so it's good to remove them. A list of stopwords can be defined by the nltk library, or it can be business-specific.\n",
        "Normalization: Normalization generally refers to a series of related tasks meant to put all text on the same level. Converting text to lower case, removing special characters, and removing stopwords will remove basic inconsistencies. Normalization improves text matching.\n",
        "Stemming: Eliminating affixes (circumfixes, suffixes, prefixes, infixes) from a word in order to obtain a word stem. Porter Stemmer is the most widely used technique because it is very fast. Generally, stemming chops off end of the word, and mostly it works fine.\n",
        "Example: Working -> Work\n",
        "Lemmatization: The goal is same as with stemming, but stemming a word sometimes loses the actual meaning of the word. Lemmatization usually refers to doing things properly using vocabulary and morphological analysis of words. It returns the base or dictionary form of a word, also known as the lemma .\n",
        "Example: Better -> Good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coXhfAuKw-WN",
        "colab_type": "text"
      },
      "source": [
        "Stemming is faster than lemmatization. You can uncomment the code and see how results change. Note: Do not apply both. Remember that stemming and lemmatization are normalization techniques, and it is recommended to use only one approach to normalize. Let your project requirements guide your decision, or you can always do experiments and see which one gives better results. In this case, stemming and lemmatizing yield almost the same accuracy.\n",
        "\n",
        "Vectorizing Data: Vectorizing is the process to convert tokens to numbers. It is an important step because the machine learning algorithm works with numbers and not text.\n",
        "In this guide, you'll implement vectorization using tf-idf. There are other techniques as well, such as Bag of Words and N-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N31cf3WwxG6w",
        "colab_type": "text"
      },
      "source": [
        "Important Note: I am using the dataset as the corpus to make a tf-idf vector. The same vector structure should be used for training and testing purposes.\n",
        "\n",
        "The target column is comprised of the integer values 0, 2, and 4. But users do not usually want their results in this form. To convert the integer results to be easily understood by users, you can implement a small script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj6QJu-4xNgH",
        "colab_type": "text"
      },
      "source": [
        "Bringing Everything Together\n",
        "In this section, we will call all the functions that you have created. You'll see Naive Bayes and Logistic Regression algorithms for predictions. These two algorithms are quite popular in NLP, although you can try out other options too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0GnY7mvxaM0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Naive Bayes is giving nearly 76% accuracy, and Logistic Regression gives nearly 79%. These accuracy figures are recorded without implementing stemming or lemmatization. Using better techniques, you might get better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo5pYrNPxYyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}